# CS76, Fall 2021, Assignment 3, Tracey Mills
## Description
Random AI simply chooses a random move from the list of possible moves.  
Minimax AI works using the minimax algorithm as described in the textbook. I chose to store max depth as an instance variable and then pass the decremented depth to each recursive call, with the cutoff reached when depth=0. I also kept only one board to conserve time and space, pushing and then popping each potential move. I implemented the iterative deepening version of minimax in the IDMinimax class. This version is identical to minimax, except that it calls the minimax with max depth ranging from 1 up to whatever depth it is called with. It stores the best move and that move's value from each call, and returns the best move overall.  
AlphaBeta AI is identical to Minimax AI, but also keeps track of the value of the worst move seen so far on the current branch. If a move is found to be worse than any move on an adjacent branch, the rest of that branch will not be explored, because the opposing player will take this worse valued move if the current player takes that branch. Thus, certainly lower utility branches need not be explored, as they would be by the Minimax AI. Because not every branch is necessarily explored, it makes sense to implement move reordering for this AI, so that likely better branches are explored first, increasing the chances that subsequent branches can be pruned. I implemented this reordering by sorting the potential moves by the utility of the resulting board, as calculated by the utility function described in the Discussion section.  

## Evaluation
The AIs all seem to work. While I do not really know how to play chess, when I played my AIs with my extremely limited knowledge, I could easily beat the random AI but the Minimax and AlphaBeta AIs gave me a challenging game. Both Minimax and AlphaBeta quickly beat the random AI. When Minimax and AlphaBeta play each other, they tie, which is unsurprising as they calculate the same utility for any given move. AlphaBeta thus gives the same valued moves as Minimax, but explores fewer nodes. For example, at the start of a new game at depth 3, minimax considers about 200,000 moves, while alphabeta considers about 25,000 moves.

## Discussion
1. When Minimax is run with max depth 1, it runs pretty much instantaneously, and beats the random AI after many moves. It makes around 10,000-30,000 recursive calls per move. When it is run with depth 2, it still chooses moves in less than a second, and beats the random AI in less moves - around 50. It makes around 600,000-700,000 recursive calls per move at this depth. At deoth 3, it takes a seconds to around a minute to move, and makes up to around 9,000,000 recursive calls per move. It beats the random AI in about 40 moves. Depth 4 takes minutes to move, and I did not complete a game at this depth.
2. My utility function gives -100 for a loss (checkmate), 0 for a tie, and uses a material evaluation function for non terminal states (values each type of piece as described in the textbook, and takes difference between own player and other player's summed piece value). The min function returns negative utility, to account for the reversed utility of the opposing player's board. Results for varied depths are described above.
3. After implementing move reordering for the Alpha-beta AI, it gives the same valued moves, but sometimes explores fewer nodes, since it is able to prune branches more often. Move reordering had more of an impact at later stages in a game. At this point, some moves are significantly better than others, and move reordering allows the better branches to be explored first, so that worse options can be pruned. This meant that especially in these later stages, when utilities between moves varied, less nodes tended to be explored when move reordering was implemented.
4. The moves reccomended by the AI in iterative deepening tended to change as the depth increased, since increased depth allowed the AI to reach later stages of the board before evalutation. This difference in evaluated boards means that moves are differently valued. For example, when playing the random AI, at one point the iterative deepening AI recommended moving its rook one square over for depths 1 and 2 - a move with value 25. However, at depth 3, it recommended moving the knight instead, a move with value 30. While for many of the early board states, the moves recommended at each of these 3 depths are the same, the difference in moves recommended at higher depths increased in the middle stages of close games, before a single player pulled ahead. At this point single moves had the power to change the evaluation of the board more, and thus seeing further ahead in the game became more valuable.
